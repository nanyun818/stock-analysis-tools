import asyncio
import os
import json
from datetime import datetime
import pandas as pd
import tushare as ts
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction import JsonCssExtractionStrategy


class FinanceCrawler:
    def __init__(self, tushare_token=None):
        """åˆå§‹åŒ–è´¢ç»çˆ¬è™«
        
        Args:
            tushare_token: Tushare Proçš„API token
        """
        self.tushare_token = tushare_token
        self.pro = None
        self.output_dir = 'finance_data'
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def login_tushare(self):
        """ç™»å½•Tushare Pro API"""
        if self.tushare_token:
            ts.set_token(self.tushare_token)
            self.pro = ts.pro_api()
            print("æˆåŠŸç™»å½•Tushare Pro API")
            return True
        else:
            print("æœªæä¾›Tushare Pro API tokenï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")
            return False
    
    async def crawl_stock_news(self, stock_code=None, pages=1):
        """çˆ¬å–è‚¡ç¥¨ç›¸å…³æ–°é—»
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœä¸ºNoneåˆ™çˆ¬å–è´¢ç»é¦–é¡µæ–°é—»
            pages: çˆ¬å–çš„é¡µé¢æ•°é‡
        """
        # æ ¹æ®æ˜¯å¦æœ‰è‚¡ç¥¨ä»£ç å†³å®šçˆ¬å–çš„URL
        if stock_code:
            # è¿™é‡Œä½¿ç”¨ä¸œæ–¹è´¢å¯Œç½‘çš„è‚¡ç¥¨æ–°é—»é¡µé¢ï¼Œå®é™…URLå¯èƒ½éœ€è¦è°ƒæ•´
            base_url = f"http://finance.eastmoney.com/a/{stock_code}.html"
        else:
            # ä½¿ç”¨æ–°æµªè´¢ç»é¦–é¡µ
            base_url = "https://finance.sina.com.cn/"
        
        # åˆ›å»ºçˆ¬è™«é…ç½®
        config = CrawlerRunConfig(
            cache_mode="memory",  # ä½¿ç”¨å†…å­˜ç¼“å­˜
            browser_options={"headless": True},  # æ— å¤´æµè§ˆå™¨æ¨¡å¼
            extraction_strategy=JsonCssExtractionStrategy({
                "news": {
                    "items": ".news-item",  # æ–°é—»é¡¹çš„CSSé€‰æ‹©å™¨
                    "fields": {
                        "title": ".news-title",
                        "summary": ".news-summary",
                        "time": ".news-time",
                        "source": ".news-source",
                        "url": {"selector": "a", "attribute": "href"}
                    }
                }
            })
        )
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler() as crawler:
            print(f"æ­£åœ¨çˆ¬å– {base_url} çš„æ–°é—»...")
            result = await crawler.arun(url=base_url, config=config)
            
            # ä¿å­˜æå–çš„æ•°æ®
            if result.extracted_data and 'news' in result.extracted_data:
                news_data = result.extracted_data['news']
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                file_name = f"news_{stock_code or 'finance'}_{timestamp}.json"
                file_path = os.path.join(self.output_dir, file_name)
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(news_data, f, ensure_ascii=False, indent=2)
                print(f"æ–°é—»æ•°æ®å·²ä¿å­˜è‡³ {file_path}")
                
                # åŒæ—¶ä¿å­˜Markdownæ ¼å¼çš„å†…å®¹
                md_file_path = os.path.join(self.output_dir, f"news_{stock_code or 'finance'}_{timestamp}.md")
                with open(md_file_path, 'w', encoding='utf-8') as f:
                    f.write(f"# è´¢ç»æ–°é—» - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                    for item in news_data:
                        f.write(f"## {item.get('title', 'æ— æ ‡é¢˜')}\n")
                        f.write(f"æ¥æº: {item.get('source', 'æœªçŸ¥')} | æ—¶é—´: {item.get('time', 'æœªçŸ¥')}\n\n")
                        f.write(f"{item.get('summary', 'æ— æ‘˜è¦')}\n\n")
                        f.write(f"[é˜…è¯»å…¨æ–‡]({item.get('url', '#')})\n\n---\n\n")
                print(f"æ–°é—»æ•°æ®(Markdownæ ¼å¼)å·²ä¿å­˜è‡³ {md_file_path}")
                
                return news_data
            else:
                print("æœªèƒ½æå–åˆ°æ–°é—»æ•°æ®")
                return None
    
    async def crawl_stock_forum(self, stock_code):
        """çˆ¬å–è‚¡ç¥¨è®ºå›è®¨è®º
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
        """
        # è¿™é‡Œä½¿ç”¨ä¸œæ–¹è´¢å¯Œè‚¡å§ä½œä¸ºç¤ºä¾‹
        url = f"http://guba.eastmoney.com/list,{stock_code}.html"
        
        # åˆ›å»ºçˆ¬è™«é…ç½®
        config = CrawlerRunConfig(
            cache_mode="memory",
            browser_options={"headless": True},
            extraction_strategy=JsonCssExtractionStrategy({
                "posts": {
                    "items": ".article_item",  # å¸–å­é¡¹çš„CSSé€‰æ‹©å™¨
                    "fields": {
                        "title": ".article_title",
                        "author": ".article_author",
                        "time": ".article_time",
                        "views": ".article_views",
                        "url": {"selector": "a", "attribute": "href"}
                    }
                }
            })
        )
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler() as crawler:
            print(f"æ­£åœ¨çˆ¬å– {url} çš„è®ºå›è®¨è®º...")
            result = await crawler.arun(url=url, config=config)
            
            # ä¿å­˜æå–çš„æ•°æ®
            if result.extracted_data and 'posts' in result.extracted_data:
                posts_data = result.extracted_data['posts']
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                file_name = f"forum_{stock_code}_{timestamp}.json"
                file_path = os.path.join(self.output_dir, file_name)
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(posts_data, f, ensure_ascii=False, indent=2)
                print(f"è®ºå›æ•°æ®å·²ä¿å­˜è‡³ {file_path}")
                
                return posts_data
            else:
                print("æœªèƒ½æå–åˆ°è®ºå›æ•°æ®")
                return None
    
    def get_stock_data(self, stock_code, start_date=None, end_date=None):
        """è·å–è‚¡ç¥¨å†å²æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆæ ¼å¼ï¼š000001.SZï¼‰
            start_date: å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
        """
        if not self.pro:
            print("è¯·å…ˆç™»å½•Tushare Pro API")
            return None
        
        # å¦‚æœæœªæŒ‡å®šæ—¥æœŸï¼Œé»˜è®¤è·å–æœ€è¿‘ä¸€ä¸ªæœˆçš„æ•°æ®
        if not end_date:
            end_date = datetime.now().strftime('%Y%m%d')
        if not start_date:
            # ç®€å•å¤„ç†ï¼Œè·å–è¿‘30å¤©æ•°æ®
            start_date = (datetime.now().replace(day=1)).strftime('%Y%m%d')
        
        try:
            df = self.pro.daily(ts_code=stock_code, start_date=start_date, end_date=end_date)
            # ä¿å­˜æ•°æ®
            file_name = f"{stock_code.replace('.', '_')}_{start_date}_{end_date}.csv"
            file_path = os.path.join(self.output_dir, file_name)
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(f"{stock_code} çš„å†å²æ•°æ®å·²ä¿å­˜è‡³ {file_path}")
            return df
        except Exception as e:
            print(f"è·å– {stock_code} çš„å†å²æ•°æ®å¤±è´¥: {e}")
            return None
    
    async def analyze_stock_sentiment(self, stock_code):
        """åˆ†æè‚¡ç¥¨ç›¸å…³æ–°é—»å’Œè®ºå›çš„æƒ…æ„Ÿ
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
        """
        # çˆ¬å–æ–°é—»å’Œè®ºå›æ•°æ®
        news_data = await self.crawl_stock_news(stock_code)
        forum_data = await self.crawl_stock_forum(stock_code)
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æƒ…æ„Ÿåˆ†æçš„ä»£ç 
        # ç®€å•ç¤ºä¾‹ï¼šç»Ÿè®¡æ ‡é¢˜ä¸­çš„å…³é”®è¯
        positive_keywords = ['æ¶¨', 'åˆ©å¥½', 'å¢é•¿', 'çªç ´', 'æœºä¼š', 'çœ‹å¥½']
        negative_keywords = ['è·Œ', 'åˆ©ç©º', 'ä¸‹è·Œ', 'é£é™©', 'è­¦æƒ•', 'æ‹…å¿§']
        
        sentiment_results = {
            'positive_count': 0,
            'negative_count': 0,
            'neutral_count': 0,
            'total_count': 0
        }
        
        # åˆ†ææ–°é—»æƒ…æ„Ÿ
        if news_data:
            for item in news_data:
                title = item.get('title', '')
                sentiment_results['total_count'] += 1
                
                is_positive = any(keyword in title for keyword in positive_keywords)
                is_negative = any(keyword in title for keyword in negative_keywords)
                
                if is_positive and not is_negative:
                    sentiment_results['positive_count'] += 1
                elif is_negative and not is_positive:
                    sentiment_results['negative_count'] += 1
                else:
                    sentiment_results['neutral_count'] += 1
        
        # åˆ†æè®ºå›æƒ…æ„Ÿ
        if forum_data:
            for item in forum_data:
                title = item.get('title', '')
                sentiment_results['total_count'] += 1
                
                is_positive = any(keyword in title for keyword in positive_keywords)
                is_negative = any(keyword in title for keyword in negative_keywords)
                
                if is_positive and not is_negative:
                    sentiment_results['positive_count'] += 1
                elif is_negative and not is_positive:
                    sentiment_results['negative_count'] += 1
                else:
                    sentiment_results['neutral_count'] += 1
        
        # ä¿å­˜æƒ…æ„Ÿåˆ†æç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_name = f"sentiment_{stock_code}_{timestamp}.json"
        file_path = os.path.join(self.output_dir, file_name)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(sentiment_results, f, ensure_ascii=False, indent=2)
        print(f"æƒ…æ„Ÿåˆ†æç»“æœå·²ä¿å­˜è‡³ {file_path}")
        
        # æ‰“å°ç®€å•çš„æƒ…æ„Ÿåˆ†ææŠ¥å‘Š
        print("\næƒ…æ„Ÿåˆ†ææŠ¥å‘Š:")
        print(f"æ€»è®¡åˆ†ææ¡ç›®: {sentiment_results['total_count']}")
        if sentiment_results['total_count'] > 0:
            positive_percent = sentiment_results['positive_count'] / sentiment_results['total_count'] * 100
            negative_percent = sentiment_results['negative_count'] / sentiment_results['total_count'] * 100
            neutral_percent = sentiment_results['neutral_count'] / sentiment_results['total_count'] * 100
            
            print(f"ç§¯ææƒ…æ„Ÿ: {sentiment_results['positive_count']} ({positive_percent:.1f}%)")
            print(f"æ¶ˆææƒ…æ„Ÿ: {sentiment_results['negative_count']} ({negative_percent:.1f}%)")
            print(f"ä¸­æ€§æƒ…æ„Ÿ: {sentiment_results['neutral_count']} ({neutral_percent:.1f}%)")
            
            # ç®€å•çš„æƒ…æ„Ÿåˆ¤æ–­
            if positive_percent > negative_percent + 10:
                print("æ•´ä½“æƒ…æ„Ÿ: ç§¯æ ğŸ˜Š")
            elif negative_percent > positive_percent + 10:
                print("æ•´ä½“æƒ…æ„Ÿ: æ¶ˆæ ğŸ˜")
            else:
                print("æ•´ä½“æƒ…æ„Ÿ: ä¸­æ€§ ğŸ˜")
        
        return sentiment_results


async def main():
    print("è´¢ç»æ•°æ®çˆ¬è™«ä¸åˆ†æç¤ºä¾‹")
    print("-" * 40)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    token = input("è¯·è¾“å…¥ä½ çš„Tushare Pro API token (å¯ç•™ç©º): ")
    crawler = FinanceCrawler(token)
    
    # å°è¯•ç™»å½•Tushare
    crawler.login_tushare()
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. çˆ¬å–è´¢ç»æ–°é—»")
        print("2. çˆ¬å–è‚¡ç¥¨è®ºå›è®¨è®º")
        print("3. è·å–è‚¡ç¥¨å†å²æ•°æ® (éœ€è¦Tushare token)")
        print("4. è‚¡ç¥¨æƒ…æ„Ÿåˆ†æ")
        print("0. é€€å‡º")
        
        choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·: ")
        
        if choice == '1':
            stock_code = input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç (å¯ç•™ç©ºçˆ¬å–è´¢ç»é¦–é¡µ): ")
            await crawler.crawl_stock_news(stock_code or None)
        elif choice == '2':
            stock_code = input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç (å¦‚: 000001): ")
            if stock_code:
                await crawler.crawl_stock_forum(stock_code)
            else:
                print("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")
        elif choice == '3':
            if not crawler.pro:
                print("æ­¤åŠŸèƒ½éœ€è¦Tushare Pro API token")
                continue
                
            ts_code = input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç (å¦‚: 000001.SZ): ")
            start_date = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸ(YYYYMMDDï¼Œå¯ç•™ç©º): ")
            end_date = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸ(YYYYMMDDï¼Œå¯ç•™ç©º): ")
            if ts_code:
                crawler.get_stock_data(ts_code, start_date or None, end_date or None)
            else:
                print("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")
        elif choice == '4':
            stock_code = input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç (å¦‚: 000001): ")
            if stock_code:
                await crawler.analyze_stock_sentiment(stock_code)
            else:
                print("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")
        elif choice == '0':
            print("ç¨‹åºå·²é€€å‡º")
            break
        else:
            print("æ— æ•ˆçš„é€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")


if __name__ == "__main__":
    asyncio.run(main())